{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 14 \n",
    "## Chris Irwin\n",
    "### 04/19/2018\n",
    "\n",
    "<center>Experimenting with Neural Network Design using Higgs Boson Data</center>\n",
    "\n",
    "## Introduction\n",
    "On 4 July 2012, the ATLAS and CMS experiments at CERN's Large Hadron Collider announced they had each observed a new particle. This particle is consistent with the Higgs boson predicted by the Standard Model. According to the CERN or The European Organization of Nuclear Research, the Standard Model explains how the basic building blocks of matter interact. <sup>1</sup> The Higgs boson, as proposed within the Standard Model, is the simplest manifestation of the Brout-Englert-Higgs mechanism. Other types of Higgs bosons are predicted by other theories that go beyond the Standard Model.<sup>2</sup>\n",
    "\n",
    "In October of 2013 the Nobel prize in physics was awarded jointly to François Englert and Peter Higgs \"for the theoretical discovery of a mechanism that contributes to our understanding of the origin of mass of subatomic particles, and which recently was confirmed through the discovery of the predicted fundamental particle, by the ATLAS and CMS experiments at CERN's Large Hadron Collider.\" <sup>2</sup> This discovery is important because scientists believe that the Higgs boson is the particle that gives all matter its mass.<sup>3</sup>\n",
    "\n",
    "In the following case study, we will use neural networks and various experiment settings to work with data created from the Higgs Boson experiment. Using the Keras and Tensorflow packages we will show the benefit of using multiple combinations of levels and activations to attempt to find a model to validate the existence of the boson particle. To test the model's overall ability we will be using three different values. The first is an ROC value or Receiver Operationg Characteristic, this calculation is most commonly used as a way to help show the ability and performance of a binary classifier.<sup>9</sup> The second is a loss function, which will be calculated using the Keras package built in loss function of binary_crossentropy. This loss function calculates the probability is the true label, and the vien distribution is the predicted value of the current model.<sup>10</sup> The final value will be an accuracy calculation, this calculation is built into the Keras pacakge and is different from the ROC calculation because the accuracy funcation uses batchs instead of the entire data set to test results. \n",
    "\n",
    "## Methods \n",
    "### Tensorflow Package\n",
    "Tensorflow is an open source program that was developed by Google to help with the creation of neural networks.<sup>4</sup> The package the ability to leverage both the computers CPU as well as its GPU in order to speed up the process of training and testing data.\n",
    "\n",
    "### Keras Package\n",
    "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation.<sup>5</sup>The Keras package has four guiding principles which are user friendliness, modularity, easy extensibility and ability to work with Python. These guiding principles have created an easy to use package that makes creating deep learning neural networks and gives the ability for the data scientists to tune different parameters for enhanced performance.  \n",
    "\n",
    "## Building Neural Networks\n",
    "The first step in building neural networks is to add the Keras and Tensorflow packages to you project. The benefits of adding both packages are explained above in greater detail. The next step is to import a number of sub-packages which will help with creating and developing a neural network. \n",
    "\n",
    "* Sequential - Allows for individual layers to be added to a neural network.\n",
    "* Activation - Imports multiple functions that can be passed to create layers.\n",
    "* Dense - Helps to implement a complex operation of activation and kernels, which are weights matrix created by the layers\n",
    "* SGD - This package includes the optimizer for Stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cmirwin/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.optimizers import Nadam\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding the packages to the case study, the next step is import data into a pandas data frame and then reshape the data to the expected format. The reshape of the data is a very important step as the keras and tensorflow packages are extremely particular when it comes the shape of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"./HIGGS.csv\",nrows=5000000,header=None)\n",
    "test_data=pd.read_csv(\"./HIGGS.csv\",nrows=200000,header=None,skiprows=6000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array(data.loc[:,0])\n",
    "x=np.array(data.loc[:,1:])\n",
    "x_test=test_data.loc[:,1:]\n",
    "y_test=test_data.loc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This case study will take a look at number of scenarios, combinations and import totals to attempt to increase the model’s ability to correctly predict outcomes. The first combination we will try is a neural net with 5 layers and using an initializer of Uniform and the sigmoid activation function on all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=x.shape[1], kernel_initializer='uniform')) # X_train.shape[1] == 15 here\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(200, kernel_initializer='uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(200, kernel_initializer='uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(50, kernel_initializer='uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='uniform')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting up the layers of the neural network the next step is to test and train the data to see what the possible outcomes are. Additionally, the fit function that is included with the Keras packages uses Epochs. According to the Keras documentation an Epoch is an arbitrary cutoff, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation. <sup>6</sup> Also we use batch sizes of 5,000 to test the model's overall accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5000000/5000000 [==============================] - 61s 12us/step - loss: 0.6915 - acc: 0.5297\n",
      "Epoch 2/5\n",
      "5000000/5000000 [==============================] - 60s 12us/step - loss: 0.6915 - acc: 0.5298\n",
      "Epoch 3/5\n",
      "5000000/5000000 [==============================] - 61s 12us/step - loss: 0.6914 - acc: 0.5298\n",
      "Epoch 4/5\n",
      "5000000/5000000 [==============================] - 61s 12us/step - loss: 0.6914 - acc: 0.5298\n",
      "Epoch 5/5\n",
      "5000000/5000000 [==============================] - 61s 12us/step - loss: 0.6914 - acc: 0.5298\n",
      "200000/200000 [==============================] - 2s 8us/step\n",
      "0.5610751371472434\n",
      "[0.6909302428364754, 0.5333549976348877]\n"
     ]
    }
   ],
   "source": [
    "model.fit(x, y, epochs=5, batch_size=5000)\n",
    "score = model.evaluate(x_test, y_test, batch_size=5000)\n",
    "print(roc_auc_score(y_test,model.predict(x_test)))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall the neural net above is set up with four layers using a batch size of 5,000 and 5 epochs, the neural net created a model with an overall accuracy of 53.3%, with an ROC score of .5610 and a loss value of .6909 when compared to the test set, which was created earlier in the analysis. This is a very poor performing model, to improve our score the next step is to alter the layers and attempt to alter the outcome. Additionally, In the following neural net, we will increase the number of Epochs to see if there is any additional information gained from the additional runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5000000/5000000 [==============================] - 61s 12us/step - loss: 0.6914 - acc: 0.5298\n",
      "Epoch 2/15\n",
      "5000000/5000000 [==============================] - 61s 12us/step - loss: 0.6914 - acc: 0.5298\n",
      "Epoch 3/15\n",
      "5000000/5000000 [==============================] - 61s 12us/step - loss: 0.6914 - acc: 0.5298\n",
      "Epoch 4/15\n",
      "5000000/5000000 [==============================] - 61s 12us/step - loss: 0.6914 - acc: 0.5298\n",
      "Epoch 5/15\n",
      "5000000/5000000 [==============================] - 61s 12us/step - loss: 0.6914 - acc: 0.5298\n",
      "Epoch 6/15\n",
      "5000000/5000000 [==============================] - 61s 12us/step - loss: 0.6914 - acc: 0.5298\n",
      "Epoch 7/15\n",
      "5000000/5000000 [==============================] - 61s 12us/step - loss: 0.6914 - acc: 0.5298\n",
      "Epoch 8/15\n",
      "5000000/5000000 [==============================] - 62s 12us/step - loss: 0.6914 - acc: 0.5298\n",
      "Epoch 9/15\n",
      "5000000/5000000 [==============================] - 62s 12us/step - loss: 0.6914 - acc: 0.5298\n",
      "Epoch 10/15\n",
      "5000000/5000000 [==============================] - 62s 12us/step - loss: 0.6914 - acc: 0.5298\n",
      "Epoch 11/15\n",
      "5000000/5000000 [==============================] - 62s 12us/step - loss: 0.6914 - acc: 0.5298\n",
      "Epoch 12/15\n",
      "5000000/5000000 [==============================] - 62s 12us/step - loss: 0.6914 - acc: 0.5298\n",
      "Epoch 13/15\n",
      "5000000/5000000 [==============================] - 62s 12us/step - loss: 0.6914 - acc: 0.5298\n",
      "Epoch 14/15\n",
      "5000000/5000000 [==============================] - 62s 12us/step - loss: 0.6914 - acc: 0.5298\n",
      "Epoch 15/15\n",
      "5000000/5000000 [==============================] - 62s 12us/step - loss: 0.6914 - acc: 0.5298\n",
      "200000/200000 [==============================] - 2s 8us/step\n",
      "0.5730254816583903\n",
      "[0.6909645915031433, 0.5333549976348877]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=x.shape[1], kernel_initializer='uniform')) # X_train.shape[1] == 15 here\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(200, kernel_initializer='uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(200, kernel_initializer='uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(50, kernel_initializer='uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='uniform')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "model.fit(x, y, epochs=15, batch_size=5000)\n",
    "score = model.evaluate(x_test, y_test, batch_size=5000)\n",
    "print(roc_auc_score(y_test,model.predict(x_test)))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the results we see that adding Epochs did not really increase the overall accuracy of the model, but we do see an increase in the ROC score, with this additional increase we will continue to use this higher number of Epochs going forward in order to see what additional information we can glean. The next step we will try to improve the score with is to remove a single layer and attempt to see if that increases the overall model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5000000/5000000 [==============================] - 57s 11us/step - loss: 0.6923 - acc: 0.5261\n",
      "Epoch 2/15\n",
      "5000000/5000000 [==============================] - 57s 11us/step - loss: 0.6917 - acc: 0.5289\n",
      "Epoch 3/15\n",
      "5000000/5000000 [==============================] - 57s 11us/step - loss: 0.6915 - acc: 0.5297\n",
      "Epoch 4/15\n",
      "5000000/5000000 [==============================] - 57s 11us/step - loss: 0.6855 - acc: 0.5477\n",
      "Epoch 5/15\n",
      "5000000/5000000 [==============================] - 57s 11us/step - loss: 0.6480 - acc: 0.6182\n",
      "Epoch 6/15\n",
      "5000000/5000000 [==============================] - 57s 11us/step - loss: 0.6400 - acc: 0.6325\n",
      "Epoch 7/15\n",
      "5000000/5000000 [==============================] - 57s 11us/step - loss: 0.6351 - acc: 0.6420\n",
      "Epoch 8/15\n",
      "5000000/5000000 [==============================] - 57s 11us/step - loss: 0.6249 - acc: 0.6553\n",
      "Epoch 9/15\n",
      "5000000/5000000 [==============================] - 57s 11us/step - loss: 0.6123 - acc: 0.6677\n",
      "Epoch 10/15\n",
      "5000000/5000000 [==============================] - 57s 11us/step - loss: 0.6014 - acc: 0.6778\n",
      "Epoch 11/15\n",
      "5000000/5000000 [==============================] - 58s 12us/step - loss: 0.5954 - acc: 0.6829\n",
      "Epoch 12/15\n",
      "5000000/5000000 [==============================] - 58s 12us/step - loss: 0.5915 - acc: 0.6860\n",
      "Epoch 13/15\n",
      "5000000/5000000 [==============================] - 58s 12us/step - loss: 0.5878 - acc: 0.6886\n",
      "Epoch 14/15\n",
      "5000000/5000000 [==============================] - 58s 12us/step - loss: 0.5847 - acc: 0.6910\n",
      "Epoch 15/15\n",
      "5000000/5000000 [==============================] - 58s 12us/step - loss: 0.5815 - acc: 0.6934\n",
      "200000/200000 [==============================] - 1s 7us/step\n",
      "0.7654891441877527\n",
      "[0.5766516178846359, 0.6966700002551078]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=x.shape[1], kernel_initializer='uniform')) # X_train.shape[1] == 15 here\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(200, kernel_initializer='uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(200, kernel_initializer='uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='uniform')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "model.fit(x, y, epochs=15, batch_size=5000)\n",
    "score = model.evaluate(x_test, y_test, batch_size=5000)\n",
    "print(roc_auc_score(y_test,model.predict(x_test)))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing a single the layer with a node size of fifty we see a considerable increase in the model’s overall accuracy, which is an increase of sixteen percentage points, .192 increase in ROC score and a .12 decrease in the loss value. With this in mind the next model we will run we will decrease the node size per layer from 200 to 150 to see what the overall effect is to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5000000/5000000 [==============================] - 42s 8us/step - loss: 0.6918 - acc: 0.5291\n",
      "Epoch 2/15\n",
      "5000000/5000000 [==============================] - 42s 8us/step - loss: 0.6916 - acc: 0.5295\n",
      "Epoch 3/15\n",
      "5000000/5000000 [==============================] - 42s 8us/step - loss: 0.6916 - acc: 0.5296\n",
      "Epoch 4/15\n",
      "5000000/5000000 [==============================] - 42s 8us/step - loss: 0.6914 - acc: 0.5297\n",
      "Epoch 5/15\n",
      "5000000/5000000 [==============================] - 42s 8us/step - loss: 0.6876 - acc: 0.5419\n",
      "Epoch 6/15\n",
      "5000000/5000000 [==============================] - 42s 8us/step - loss: 0.6498 - acc: 0.6157\n",
      "Epoch 7/15\n",
      "5000000/5000000 [==============================] - 42s 8us/step - loss: 0.6397 - acc: 0.6328\n",
      "Epoch 8/15\n",
      "5000000/5000000 [==============================] - 42s 8us/step - loss: 0.6327 - acc: 0.6452\n",
      "Epoch 9/15\n",
      "5000000/5000000 [==============================] - 42s 8us/step - loss: 0.6209 - acc: 0.6592\n",
      "Epoch 10/15\n",
      "5000000/5000000 [==============================] - 42s 8us/step - loss: 0.6076 - acc: 0.6720\n",
      "Epoch 11/15\n",
      "5000000/5000000 [==============================] - 42s 8us/step - loss: 0.5983 - acc: 0.6807\n",
      "Epoch 12/15\n",
      "5000000/5000000 [==============================] - 42s 8us/step - loss: 0.5927 - acc: 0.6853\n",
      "Epoch 13/15\n",
      "5000000/5000000 [==============================] - 42s 8us/step - loss: 0.5886 - acc: 0.6883\n",
      "Epoch 14/15\n",
      "5000000/5000000 [==============================] - 42s 8us/step - loss: 0.5849 - acc: 0.6910\n",
      "Epoch 15/15\n",
      "5000000/5000000 [==============================] - 42s 8us/step - loss: 0.5815 - acc: 0.6935\n",
      "200000/200000 [==============================] - 1s 5us/step\n",
      "0.7655810422045217\n",
      "[0.5763749212026597, 0.6988150030374527]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(150, input_dim=x.shape[1], kernel_initializer='uniform')) # X_train.shape[1] == 15 here\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(150, kernel_initializer='uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(150, kernel_initializer='uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='uniform')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "model.fit(x, y, epochs=15, batch_size=5000)\n",
    "score = model.evaluate(x_test, y_test, batch_size=5000)\n",
    "print(roc_auc_score(y_test,model.predict(x_test)))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results from the model which was run above we find that by decreasing the node size of each layer creates a similar overall accuracy, ROC score and loss value when applied to the test data set. The interesting part in this section of the analysis is that similar results were achieved using a lower node count, but the smaller nodes sizes ran in a considerably less amount of time by averaging approximately 18-20 seconds less per run. \n",
    "\n",
    "## Alter Activation Methods\n",
    "\n",
    "The next step in this case study will be to examine the usage of different activations. Using the model above that had the highest accuracy, which was the model with 3 layers of 200 per node, we will apply multiple activations in an attempt to raise the score. Keras gives access to a number of different activations and we will focus specifically on the relu, tanh and linear activations. Additionally, the other two models are subjected to the same process of altering the activations with code and results available in the appendix located at the end of this case study.\n",
    "\n",
    "### relu Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5000000/5000000 [==============================] - 30s 6us/step - loss: 0.6164 - acc: 0.6492\n",
      "Epoch 2/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.5550 - acc: 0.7116\n",
      "Epoch 3/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.5297 - acc: 0.7307\n",
      "Epoch 4/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.5137 - acc: 0.7414\n",
      "Epoch 5/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.5022 - acc: 0.7489\n",
      "Epoch 6/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4949 - acc: 0.7537\n",
      "Epoch 7/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4896 - acc: 0.7572\n",
      "Epoch 8/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4861 - acc: 0.7596\n",
      "Epoch 9/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4829 - acc: 0.7615\n",
      "Epoch 10/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4805 - acc: 0.7630\n",
      "Epoch 11/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4785 - acc: 0.7645\n",
      "Epoch 12/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4767 - acc: 0.7657\n",
      "Epoch 13/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4750 - acc: 0.7667\n",
      "Epoch 14/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4737 - acc: 0.7675\n",
      "Epoch 15/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4724 - acc: 0.7684\n",
      "200000/200000 [==============================] - 0s 2us/step\n",
      "0.852819234661032\n",
      "[0.472449616342783, 0.7687800034880639]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=x.shape[1], kernel_initializer='uniform')) # X_train.shape[1] == 15 here\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200, kernel_initializer='uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200, kernel_initializer='uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "model.fit(x, y, epochs=15, batch_size=5000)\n",
    "score = model.evaluate(x_test, y_test, batch_size=5000)\n",
    "print(roc_auc_score(y_test,model.predict(x_test)))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above are very compelling, by removing the sigmoid activation and replacing it with relu we found there to be a large increase in the ROC score, a seven percentage point increase in accuracy and a .10 decrease in the loss value. An additional benefit to the relu function also seems to be an overall decrease in the amount of run time per Epoch of almost 30 seconds per run.\n",
    "\n",
    "Continuing the attempt to improve the score of the model we will next compare the results of the model when we change the relu activation for the tanh activation to see if there is any increase in accruacy or decrease in run time. \n",
    "\n",
    "### tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5000000/5000000 [==============================] - 55s 11us/step - loss: 0.6375 - acc: 0.6344\n",
      "Epoch 2/15\n",
      "5000000/5000000 [==============================] - 55s 11us/step - loss: 0.5968 - acc: 0.6791\n",
      "Epoch 3/15\n",
      "5000000/5000000 [==============================] - 55s 11us/step - loss: 0.5785 - acc: 0.6933\n",
      "Epoch 4/15\n",
      "5000000/5000000 [==============================] - 55s 11us/step - loss: 0.5654 - acc: 0.7032\n",
      "Epoch 5/15\n",
      "5000000/5000000 [==============================] - 56s 11us/step - loss: 0.5552 - acc: 0.7110\n",
      "Epoch 6/15\n",
      "5000000/5000000 [==============================] - 55s 11us/step - loss: 0.5477 - acc: 0.7167\n",
      "Epoch 7/15\n",
      "5000000/5000000 [==============================] - 55s 11us/step - loss: 0.5414 - acc: 0.7216\n",
      "Epoch 8/15\n",
      "5000000/5000000 [==============================] - 56s 11us/step - loss: 0.5356 - acc: 0.7259\n",
      "Epoch 9/15\n",
      "5000000/5000000 [==============================] - 56s 11us/step - loss: 0.5304 - acc: 0.7297\n",
      "Epoch 10/15\n",
      "5000000/5000000 [==============================] - 56s 11us/step - loss: 0.5251 - acc: 0.7334\n",
      "Epoch 11/15\n",
      "5000000/5000000 [==============================] - 56s 11us/step - loss: 0.5206 - acc: 0.7366\n",
      "Epoch 12/15\n",
      "5000000/5000000 [==============================] - 56s 11us/step - loss: 0.5170 - acc: 0.7391\n",
      "Epoch 13/15\n",
      "5000000/5000000 [==============================] - 56s 11us/step - loss: 0.5134 - acc: 0.7417\n",
      "Epoch 14/15\n",
      "5000000/5000000 [==============================] - 56s 11us/step - loss: 0.5102 - acc: 0.7440\n",
      "Epoch 15/15\n",
      "5000000/5000000 [==============================] - 56s 11us/step - loss: 0.5075 - acc: 0.7457\n",
      "200000/200000 [==============================] - 1s 7us/step\n",
      "0.8301749923582097\n",
      "[0.5036467663943768, 0.7492049977183342]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=x.shape[1], kernel_initializer='uniform')) # X_train.shape[1] == 15 here\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(200, kernel_initializer='uniform'))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(200, kernel_initializer='uniform'))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(1, kernel_initializer='uniform')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "model.fit(x, y, epochs=15, batch_size=5000)\n",
    "score = model.evaluate(x_test, y_test, batch_size=5000)\n",
    "print(roc_auc_score(y_test,model.predict(x_test)))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the tanh are still an overall improvement over the use of sigmoid activations with an overall increase in the ROC Score, an increase of 14 percentage points in accuracy and decrease in the loss value of .7. When compared to the relu result the tanh there is a glaring increase in the run time with each run lasting approximently 55 seconds on average, with an overall decrease in accuracy and overall increase in loss. \n",
    "\n",
    "Next we will look at the linear activation to see how it performs and if there is any increase in overall model percision. \n",
    "\n",
    "### linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5000000/5000000 [==============================] - 28s 6us/step - loss: 0.6445 - acc: 0.6268\n",
      "Epoch 2/15\n",
      "5000000/5000000 [==============================] - 29s 6us/step - loss: 0.6385 - acc: 0.6395\n",
      "Epoch 3/15\n",
      "5000000/5000000 [==============================] - 29s 6us/step - loss: 0.6384 - acc: 0.6400\n",
      "Epoch 4/15\n",
      "5000000/5000000 [==============================] - 29s 6us/step - loss: 0.6383 - acc: 0.6401\n",
      "Epoch 5/15\n",
      "5000000/5000000 [==============================] - 29s 6us/step - loss: 0.6383 - acc: 0.6404\n",
      "Epoch 6/15\n",
      "5000000/5000000 [==============================] - 29s 6us/step - loss: 0.6382 - acc: 0.6403\n",
      "Epoch 7/15\n",
      "5000000/5000000 [==============================] - 29s 6us/step - loss: 0.6382 - acc: 0.6404\n",
      "Epoch 8/15\n",
      "5000000/5000000 [==============================] - 29s 6us/step - loss: 0.6382 - acc: 0.6405\n",
      "Epoch 9/15\n",
      "5000000/5000000 [==============================] - 29s 6us/step - loss: 0.6382 - acc: 0.6405\n",
      "Epoch 10/15\n",
      "5000000/5000000 [==============================] - 29s 6us/step - loss: 0.6382 - acc: 0.6405\n",
      "Epoch 11/15\n",
      "5000000/5000000 [==============================] - 29s 6us/step - loss: 0.6381 - acc: 0.6405\n",
      "Epoch 12/15\n",
      "5000000/5000000 [==============================] - 29s 6us/step - loss: 0.6381 - acc: 0.6405\n",
      "Epoch 13/15\n",
      "5000000/5000000 [==============================] - 29s 6us/step - loss: 0.6381 - acc: 0.6406\n",
      "Epoch 14/15\n",
      "5000000/5000000 [==============================] - 29s 6us/step - loss: 0.6381 - acc: 0.6407\n",
      "Epoch 15/15\n",
      "5000000/5000000 [==============================] - 29s 6us/step - loss: 0.6381 - acc: 0.6407\n",
      "200000/200000 [==============================] - 0s 2us/step\n",
      "0.6849953880342189\n",
      "[0.6369215965270996, 0.6425300002098083]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=x.shape[1], kernel_initializer='uniform')) # X_train.shape[1] == 15 here\n",
    "model.add(Activation('linear'))\n",
    "model.add(Dense(200, kernel_initializer='uniform'))\n",
    "model.add(Activation('linear'))\n",
    "model.add(Dense(200, kernel_initializer='uniform'))\n",
    "model.add(Activation('linear'))\n",
    "model.add(Dense(1, kernel_initializer='uniform')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#rms = RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=1e-6)\n",
    "#ada = Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "#nad = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "model.fit(x, y, epochs=15, batch_size=5000)\n",
    "score = model.evaluate(x_test, y_test, batch_size=5000)\n",
    "print(roc_auc_score(y_test,model.predict(x_test)))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final activation that we attempted had results that were the lowest amongst the three activations we tried. Even when compared to the original model with sigmoid activation we find that the model’s accuracy decreased by approximately five percentage points and gained approximately .07 in loss value. Interestingly the run times per Epoch improved considerably and were the best out of all the activations used. \n",
    "\n",
    "## Altering Batch Sizes\n",
    "\n",
    "After finding that the relu activation is gives us the highest accuracy the next item to see if we can increase the overall accuracy is to increase and decrease the total batch size. We will run two different scenarios with batch sizes of 25,000 records a batch of 75. \n",
    "\n",
    "### Batch of 25,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5000000/5000000 [==============================] - 34s 7us/step - loss: 0.6831 - acc: 0.5513\n",
      "Epoch 2/15\n",
      "5000000/5000000 [==============================] - 35s 7us/step - loss: 0.6275 - acc: 0.6451\n",
      "Epoch 3/15\n",
      "5000000/5000000 [==============================] - 35s 7us/step - loss: 0.6003 - acc: 0.6753\n",
      "Epoch 4/15\n",
      "5000000/5000000 [==============================] - 35s 7us/step - loss: 0.5848 - acc: 0.6882\n",
      "Epoch 5/15\n",
      "5000000/5000000 [==============================] - 34s 7us/step - loss: 0.5722 - acc: 0.6980\n",
      "Epoch 6/15\n",
      "5000000/5000000 [==============================] - 34s 7us/step - loss: 0.5637 - acc: 0.7047\n",
      "Epoch 7/15\n",
      "5000000/5000000 [==============================] - 35s 7us/step - loss: 0.5568 - acc: 0.7103\n",
      "Epoch 8/15\n",
      "5000000/5000000 [==============================] - 35s 7us/step - loss: 0.5495 - acc: 0.7160\n",
      "Epoch 9/15\n",
      "5000000/5000000 [==============================] - 34s 7us/step - loss: 0.5427 - acc: 0.7210\n",
      "Epoch 10/15\n",
      "5000000/5000000 [==============================] - 34s 7us/step - loss: 0.5366 - acc: 0.7255\n",
      "Epoch 11/15\n",
      "5000000/5000000 [==============================] - 34s 7us/step - loss: 0.5315 - acc: 0.7289\n",
      "Epoch 12/15\n",
      "5000000/5000000 [==============================] - 34s 7us/step - loss: 0.5261 - acc: 0.7328\n",
      "Epoch 13/15\n",
      "5000000/5000000 [==============================] - 34s 7us/step - loss: 0.5213 - acc: 0.7361\n",
      "Epoch 14/15\n",
      "5000000/5000000 [==============================] - 34s 7us/step - loss: 0.5171 - acc: 0.7391\n",
      "Epoch 15/15\n",
      "5000000/5000000 [==============================] - 34s 7us/step - loss: 0.5133 - acc: 0.7415\n",
      "200000/200000 [==============================] - 0s 2us/step\n",
      "0.8284343653780737\n",
      "[0.517031840980053, 0.7381500005722046]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=x.shape[1], kernel_initializer='uniform')) # X_train.shape[1] == 15 here\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200, kernel_initializer='uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200, kernel_initializer='uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "model.fit(x, y, epochs=15, batch_size=25000)\n",
    "score = model.evaluate(x_test, y_test, batch_size=25000)\n",
    "print(roc_auc_score(y_test,model.predict(x_test)))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches of 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5000000/5000000 [==============================] - 98s 20us/step - loss: 0.5594 - acc: 0.7098\n",
      "Epoch 2/15\n",
      "5000000/5000000 [==============================] - 107s 21us/step - loss: 0.5299 - acc: 0.7336\n",
      "Epoch 3/15\n",
      "5000000/5000000 [==============================] - 111s 22us/step - loss: 0.5192 - acc: 0.7410\n",
      "Epoch 4/15\n",
      "5000000/5000000 [==============================] - 114s 23us/step - loss: 0.5128 - acc: 0.7454\n",
      "Epoch 5/15\n",
      "5000000/5000000 [==============================] - 116s 23us/step - loss: 0.5082 - acc: 0.7480\n",
      "Epoch 6/15\n",
      "5000000/5000000 [==============================] - 115s 23us/step - loss: 0.5047 - acc: 0.7504\n",
      "Epoch 7/15\n",
      "5000000/5000000 [==============================] - 116s 23us/step - loss: 0.5017 - acc: 0.7523\n",
      "Epoch 8/15\n",
      "5000000/5000000 [==============================] - 115s 23us/step - loss: 0.4990 - acc: 0.7540\n",
      "Epoch 9/15\n",
      "5000000/5000000 [==============================] - 116s 23us/step - loss: 0.4968 - acc: 0.7554\n",
      "Epoch 10/15\n",
      "5000000/5000000 [==============================] - 115s 23us/step - loss: 0.4946 - acc: 0.7570\n",
      "Epoch 11/15\n",
      "5000000/5000000 [==============================] - 116s 23us/step - loss: 0.4929 - acc: 0.7580\n",
      "Epoch 12/15\n",
      "5000000/5000000 [==============================] - 115s 23us/step - loss: 0.4911 - acc: 0.7591\n",
      "Epoch 13/15\n",
      "5000000/5000000 [==============================] - 115s 23us/step - loss: 0.4896 - acc: 0.7601\n",
      "Epoch 14/15\n",
      "5000000/5000000 [==============================] - 114s 23us/step - loss: 0.4880 - acc: 0.7610\n",
      "Epoch 15/15\n",
      "5000000/5000000 [==============================] - 114s 23us/step - loss: 0.4867 - acc: 0.7617\n",
      "200000/200000 [==============================] - 1s 4us/step\n",
      "0.8448842197665147\n",
      "[0.48791625937446953, 0.7626800020560622]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=x.shape[1], kernel_initializer='uniform')) # X_train.shape[1] == 15 here\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200, kernel_initializer='uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200, kernel_initializer='uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "model.fit(x, y, epochs=15, batch_size=75)\n",
    "score = model.evaluate(x_test, y_test, batch_size=75)\n",
    "print(roc_auc_score(y_test,model.predict(x_test)))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running two different batch sizes we can see that by increasing the batch size to 25,000 records per batch decreased the model’s accuracy by approximately 2 percentage points and increased the loss value by approximately .03. Additionally, decreasing the batch size to 75 slightly decreased the accuracy by .6 percentage point and slightly increased the loss value. Overall the performance was very similar between the batches of 75 vs 5,000, but the run time between the two is dramatically different with the batches of 75 taking on average 2 minutes per Epoch or an increase of 150%.\n",
    "\n",
    "## Altering Kernal Initializers\n",
    "\n",
    "As we explained earlier in this case study there are a number of different variables that can be altered to increase the accuracy and effectiveness of the model. These values are available because of the different type data that can be ran through the neural net process, whether the data is image related, scientific results or sports statistics different data requires different algorithms to get the best results. According to the Keras documentation the Initializations define the way to set the initial random weights of Keras layers. <sup>7</sup>\n",
    "\n",
    "In the following part of the case study we will run the previous defined model with 3 layers of node sizes 200 with batch sizes of 5,000 with three different initializer values. Previously in this case study we have used a constant kernel initializer of uniform and have achieved very good overall results. Now we will run the same model and compare the results by changing the kernel initializer to use either orthogonal, VarianceScaling or truncated_normal initializer values.\n",
    "\n",
    "### Orthogonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5000000/5000000 [==============================] - 30s 6us/step - loss: 0.5923 - acc: 0.6776\n",
      "Epoch 2/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.5391 - acc: 0.7241\n",
      "Epoch 3/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.5143 - acc: 0.7411\n",
      "Epoch 4/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.5013 - acc: 0.7496\n",
      "Epoch 5/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4943 - acc: 0.7539\n",
      "Epoch 6/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4896 - acc: 0.7571\n",
      "Epoch 7/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4861 - acc: 0.7595\n",
      "Epoch 8/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4834 - acc: 0.7608\n",
      "Epoch 9/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4811 - acc: 0.7626\n",
      "Epoch 10/15\n",
      "5000000/5000000 [==============================] - 33s 7us/step - loss: 0.4790 - acc: 0.7639\n",
      "Epoch 11/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4773 - acc: 0.7650\n",
      "Epoch 12/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4757 - acc: 0.7661\n",
      "Epoch 13/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4744 - acc: 0.7668\n",
      "Epoch 14/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4732 - acc: 0.7677\n",
      "Epoch 15/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4720 - acc: 0.7685\n",
      "200000/200000 [==============================] - 0s 2us/step\n",
      "0.8525290446002298\n",
      "[0.473652059584856, 0.7681900024414062]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=x.shape[1], kernel_initializer='orthogonal')) # X_train.shape[1] == 15 here\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200, kernel_initializer='orthogonal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200, kernel_initializer='orthogonal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='orthogonal')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "model.fit(x, y, epochs=15, batch_size=5000)\n",
    "score = model.evaluate(x_test, y_test, batch_size=5000)\n",
    "print(roc_auc_score(y_test,model.predict(x_test)))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After updating the kernel initializer value to Orthogonal, we see the results are very similar to the when we ran the process with the Uniform kernel initializer value. All four categories, ROC Score, accuracy, loss value and time per Epoch show little difference between the two initializer values.\n",
    "\n",
    "### VarianceScaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5000000/5000000 [==============================] - 30s 6us/step - loss: 0.5942 - acc: 0.6760\n",
      "Epoch 2/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.5437 - acc: 0.7205\n",
      "Epoch 3/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.5207 - acc: 0.7371\n",
      "Epoch 4/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.5067 - acc: 0.7462\n",
      "Epoch 5/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4980 - acc: 0.7518\n",
      "Epoch 6/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4926 - acc: 0.7552\n",
      "Epoch 7/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4889 - acc: 0.7577\n",
      "Epoch 8/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4858 - acc: 0.7596\n",
      "Epoch 9/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4834 - acc: 0.7611\n",
      "Epoch 10/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4815 - acc: 0.7625\n",
      "Epoch 11/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4796 - acc: 0.7637\n",
      "Epoch 12/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4781 - acc: 0.7646\n",
      "Epoch 13/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4766 - acc: 0.7657\n",
      "Epoch 14/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4753 - acc: 0.7663\n",
      "Epoch 15/15\n",
      "5000000/5000000 [==============================] - 32s 6us/step - loss: 0.4741 - acc: 0.7672\n",
      "200000/200000 [==============================] - 1s 3us/step\n",
      "0.8500474277993356\n",
      "[0.4765773430466652, 0.7669149965047837]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=x.shape[1], kernel_initializer='VarianceScaling')) # X_train.shape[1] == 15 here\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200, kernel_initializer='VarianceScaling'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200, kernel_initializer='VarianceScaling'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='VarianceScaling')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "model.fit(x, y, epochs=15, batch_size=5000)\n",
    "score = model.evaluate(x_test, y_test, batch_size=5000)\n",
    "print(roc_auc_score(y_test,model.predict(x_test)))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step of the analysis is to change Orthogonal to an initialization value of VarianceScaling. Once again, we see a very minimal amount of difference between the results of both the Uniform, Orthogonal and VarianceScaling. Overall we see a slight descrease in ROC Score, a descrease in accuracy and a slight increase in loss value. \n",
    "\n",
    "\n",
    "### Truncated_Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5000000/5000000 [==============================] - 38s 8us/step - loss: 0.5956 - acc: 0.6726\n",
      "Epoch 2/15\n",
      "5000000/5000000 [==============================] - 38s 8us/step - loss: 0.5373 - acc: 0.7251\n",
      "Epoch 3/15\n",
      "5000000/5000000 [==============================] - 37s 7us/step - loss: 0.5129 - acc: 0.7416\n",
      "Epoch 4/15\n",
      "5000000/5000000 [==============================] - 37s 7us/step - loss: 0.5000 - acc: 0.7502\n",
      "Epoch 5/15\n",
      "5000000/5000000 [==============================] - 33s 7us/step - loss: 0.4928 - acc: 0.7550\n",
      "Epoch 6/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4877 - acc: 0.7582\n",
      "Epoch 7/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4844 - acc: 0.7603\n",
      "Epoch 8/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4815 - acc: 0.7622\n",
      "Epoch 9/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4791 - acc: 0.7637\n",
      "Epoch 10/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4772 - acc: 0.7651\n",
      "Epoch 11/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4753 - acc: 0.7663\n",
      "Epoch 12/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4738 - acc: 0.7673\n",
      "Epoch 13/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4724 - acc: 0.7682\n",
      "Epoch 14/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4710 - acc: 0.7691\n",
      "Epoch 15/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4698 - acc: 0.7698\n",
      "200000/200000 [==============================] - 0s 2us/step\n",
      "0.8544244084440862\n",
      "[0.4704068087041378, 0.7702399984002113]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=x.shape[1], kernel_initializer='truncated_normal')) # X_train.shape[1] == 15 here\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200, kernel_initializer='truncated_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200, kernel_initializer='truncated_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='truncated_normal')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "model.fit(x, y, epochs=15, batch_size=5000)\n",
    "score = model.evaluate(x_test, y_test, batch_size=5000)\n",
    "print(roc_auc_score(y_test,model.predict(x_test)))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final initializer value we attempted is the truncated_normal. This value gave us our first lowering of the loss value by approximately .05 when compared to all three of the previously used initializer values. Additionally, we saw an increase in the model’s accuracy with an approximately 1 percentage point and an increase in ROC Score. Moving forward to the next step of our case study we will use an initialization value of truncated_normal\n",
    "\n",
    "\n",
    "## Altering Neural Net Optimizers \n",
    "Finally, after altering layer sizes and counts, changing activation types, testing different batch sizes and running multiple kernels we are going to attempt to improve the accuracy of the model with trying different optimizers. The choice of optimizer is an incredibly important one as it will commonly affect the overall performance of your model. Below we will attempt to build the model using three specific RMSProp, Nadam and Adagrad\n",
    "\n",
    "*RMSProp - RMSprop is an unpublished, adaptive learning rate method proposed by Geoff Hinton in Lecture 6e of his Coursera Class. <sup>8</sup>\n",
    "\n",
    "*Nadam – Is similar to RMSProp with an adjustment for momentum by using the Nesterov accelerated gradient <sup>8</sup>\n",
    "\n",
    "*Adagrad - is an algorithm for gradient-based optimization that does just this: It adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters. <sup>8</sup>\n",
    "\n",
    "### RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5000000/5000000 [==============================] - 34s 7us/step - loss: 0.6273 - acc: 0.6448\n",
      "Epoch 2/15\n",
      "5000000/5000000 [==============================] - 34s 7us/step - loss: 0.5710 - acc: 0.7039\n",
      "Epoch 3/15\n",
      "5000000/5000000 [==============================] - 35s 7us/step - loss: 0.5503 - acc: 0.7210\n",
      "Epoch 4/15\n",
      "5000000/5000000 [==============================] - 35s 7us/step - loss: 0.5381 - acc: 0.7303\n",
      "Epoch 5/15\n",
      "5000000/5000000 [==============================] - 35s 7us/step - loss: 0.5314 - acc: 0.7358\n",
      "Epoch 6/15\n",
      "5000000/5000000 [==============================] - 35s 7us/step - loss: 0.5274 - acc: 0.7395\n",
      "Epoch 7/15\n",
      "5000000/5000000 [==============================] - 35s 7us/step - loss: 0.5243 - acc: 0.7421\n",
      "Epoch 8/15\n",
      "5000000/5000000 [==============================] - 35s 7us/step - loss: 0.5217 - acc: 0.7443\n",
      "Epoch 9/15\n",
      "5000000/5000000 [==============================] - 35s 7us/step - loss: 0.5216 - acc: 0.7456\n",
      "Epoch 10/15\n",
      "5000000/5000000 [==============================] - 35s 7us/step - loss: 0.5200 - acc: 0.7462\n",
      "Epoch 11/15\n",
      "5000000/5000000 [==============================] - 35s 7us/step - loss: 0.5183 - acc: 0.7476\n",
      "Epoch 12/15\n",
      "5000000/5000000 [==============================] - 35s 7us/step - loss: 0.5170 - acc: 0.7488\n",
      "Epoch 13/15\n",
      "5000000/5000000 [==============================] - 35s 7us/step - loss: 0.5196 - acc: 0.7485\n",
      "Epoch 14/15\n",
      "5000000/5000000 [==============================] - 35s 7us/step - loss: 0.5223 - acc: 0.7479\n",
      "Epoch 15/15\n",
      "5000000/5000000 [==============================] - 36s 7us/step - loss: 0.5209 - acc: 0.7487\n",
      "200000/200000 [==============================] - 0s 2us/step\n",
      "0.8335705549727903\n",
      "[0.5254958048462868, 0.7487200036644935]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=x.shape[1], kernel_initializer='truncated_normal')) # X_train.shape[1] == 15 here\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200, kernel_initializer='truncated_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200, kernel_initializer='truncated_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='truncated_normal')) \n",
    "model.add(Activation('relu'))\n",
    "\n",
    "rms = RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=1e-6)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=rms)\n",
    "\n",
    "model.fit(x, y, epochs=15, batch_size=5000)\n",
    "score = model.evaluate(x_test, y_test, batch_size=5000)\n",
    "print(roc_auc_score(y_test,model.predict(x_test)))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we altered the optimizer to be RMSProp we found that there was a decrease in ROC Score of approximately .02, a decrease in accuracy of approximately .02, an increase in loss function of approximately .05 and finally an increase in the amount of time it took for each Epoch to run. \n",
    "\n",
    "Next we tested the optimizer of Adagrad to see what effect it has on the models performance.\n",
    "### Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5000000/5000000 [==============================] - 35s 7us/step - loss: 0.5903 - acc: 0.6784\n",
      "Epoch 2/15\n",
      "5000000/5000000 [==============================] - 35s 7us/step - loss: 0.5489 - acc: 0.7168\n",
      "Epoch 3/15\n",
      "5000000/5000000 [==============================] - 35s 7us/step - loss: 0.5331 - acc: 0.7292\n",
      "Epoch 4/15\n",
      "5000000/5000000 [==============================] - 35s 7us/step - loss: 0.5236 - acc: 0.7359\n",
      "Epoch 5/15\n",
      "5000000/5000000 [==============================] - 35s 7us/step - loss: 0.5169 - acc: 0.7404\n",
      "Epoch 6/15\n",
      "5000000/5000000 [==============================] - 35s 7us/step - loss: 0.5120 - acc: 0.7435\n",
      "Epoch 7/15\n",
      "5000000/5000000 [==============================] - 35s 7us/step - loss: 0.5078 - acc: 0.7460\n",
      "Epoch 8/15\n",
      "5000000/5000000 [==============================] - 35s 7us/step - loss: 0.5045 - acc: 0.7481\n",
      "Epoch 9/15\n",
      "5000000/5000000 [==============================] - 35s 7us/step - loss: 0.5016 - acc: 0.7499\n",
      "Epoch 10/15\n",
      "5000000/5000000 [==============================] - 35s 7us/step - loss: 0.4989 - acc: 0.7517\n",
      "Epoch 11/15\n",
      "5000000/5000000 [==============================] - 35s 7us/step - loss: 0.4967 - acc: 0.7530\n",
      "Epoch 12/15\n",
      "5000000/5000000 [==============================] - 36s 7us/step - loss: 0.4946 - acc: 0.7544\n",
      "Epoch 13/15\n",
      "5000000/5000000 [==============================] - 36s 7us/step - loss: 0.4929 - acc: 0.7553\n",
      "Epoch 14/15\n",
      "5000000/5000000 [==============================] - 35s 7us/step - loss: 0.4912 - acc: 0.7564\n",
      "Epoch 15/15\n",
      "5000000/5000000 [==============================] - 36s 7us/step - loss: 0.4899 - acc: 0.7572\n",
      "200000/200000 [==============================] - 0s 2us/step\n",
      "0.8415409605136148\n",
      "[0.4891367875039577, 0.7584949985146523]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=x.shape[1], kernel_initializer='truncated_normal')) # X_train.shape[1] == 15 here\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200, kernel_initializer='truncated_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200, kernel_initializer='truncated_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='truncated_normal')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "ada = Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "#nad = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=ada)\n",
    "\n",
    "model.fit(x, y, epochs=15, batch_size=5000)\n",
    "score = model.evaluate(x_test, y_test, batch_size=5000)\n",
    "print(roc_auc_score(y_test,model.predict(x_test)))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After altering the optimizer to Adagrad and rerunning the experiment we saw a slight increase in ROC score and accuracy from the new optimizer when compared to the RMSProp run. In comparing the new optimizer to our current best test we found that there is still a decrease in ROC Score and accuracy with an increase in loss. \n",
    "\n",
    "Finally, we will run the experiment one last time to see if the Nadam optimizer has any positive effect on the overall model.\n",
    "\n",
    "### Nadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5000000/5000000 [==============================] - 37s 7us/step - loss: 0.5768 - acc: 0.6912\n",
      "Epoch 2/15\n",
      "5000000/5000000 [==============================] - 38s 8us/step - loss: 0.5211 - acc: 0.7369\n",
      "Epoch 3/15\n",
      "5000000/5000000 [==============================] - 36s 7us/step - loss: 0.5008 - acc: 0.7503\n",
      "Epoch 4/15\n",
      "5000000/5000000 [==============================] - 36s 7us/step - loss: 0.4892 - acc: 0.7574\n",
      "Epoch 5/15\n",
      "5000000/5000000 [==============================] - 36s 7us/step - loss: 0.4824 - acc: 0.7619\n",
      "Epoch 6/15\n",
      "5000000/5000000 [==============================] - 37s 7us/step - loss: 0.4775 - acc: 0.7650\n",
      "Epoch 7/15\n",
      "5000000/5000000 [==============================] - 37s 7us/step - loss: 0.4740 - acc: 0.7674\n",
      "Epoch 8/15\n",
      "5000000/5000000 [==============================] - 36s 7us/step - loss: 0.4707 - acc: 0.7695\n",
      "Epoch 9/15\n",
      "5000000/5000000 [==============================] - 36s 7us/step - loss: 0.4685 - acc: 0.7707\n",
      "Epoch 10/15\n",
      "5000000/5000000 [==============================] - 33s 7us/step - loss: 0.4664 - acc: 0.7722\n",
      "Epoch 11/15\n",
      "5000000/5000000 [==============================] - 33s 7us/step - loss: 0.4647 - acc: 0.7733\n",
      "Epoch 12/15\n",
      "5000000/5000000 [==============================] - 33s 7us/step - loss: 0.4630 - acc: 0.7743\n",
      "Epoch 13/15\n",
      "5000000/5000000 [==============================] - 34s 7us/step - loss: 0.4617 - acc: 0.7752\n",
      "Epoch 14/15\n",
      "5000000/5000000 [==============================] - 33s 7us/step - loss: 0.4604 - acc: 0.7761\n",
      "Epoch 15/15\n",
      "5000000/5000000 [==============================] - 33s 7us/step - loss: 0.4594 - acc: 0.7766\n",
      "200000/200000 [==============================] - 0s 2us/step\n",
      "0.8593213225090737\n",
      "[0.4634594313800335, 0.775160001218319]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=x.shape[1], kernel_initializer='truncated_normal')) # X_train.shape[1] == 15 here\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200, kernel_initializer='truncated_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200, kernel_initializer='truncated_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='truncated_normal')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "nad = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=nad)\n",
    "\n",
    "model.fit(x, y, epochs=15, batch_size=5000)\n",
    "score = model.evaluate(x_test, y_test, batch_size=5000)\n",
    "print(roc_auc_score(y_test,model.predict(x_test)))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Nadam optimizer produced the best results of any model we have produced so far. Additionally, it performed the best of all three of the optimizers with an overall ROC score of .859 and an accuracy of .775. When compared to the highest achieved accuracy in this case study the Nadam model increased the accuracy by approximately .05.\n",
    "\n",
    "After numerous attempts at maximizing the score of our model we have found that having the following settings has produced a very successful model.\n",
    "\n",
    "* Record Count – 5 million\n",
    "* Batch Size – 5000\n",
    "* Layers – 3\n",
    "* Node Size - 200\n",
    "* Epochs - 15\n",
    "* Kernel Initializer – truncated_normal\n",
    "* Activation – Relu\n",
    "* Optimizer – Nadam\n",
    "\n",
    "Earlier in the case study we attempted to lower the node size to see if it would help with improving the model. The results of that experiment were a slight overall change in the model’s accuracy and ROC score. In a final attempt to maximize the score we attempted to increase the size node size to see if the additional data enhanced the ability of the model. Below we will attempt two additional analysis, the first is increasing the node size to 350 and the second is increasing the node size to 750. \n",
    "\n",
    "\n",
    "### Nadam with Nodes of 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5000000/5000000 [==============================] - 89s 18us/step - loss: 0.5721 - acc: 0.6948\n",
      "Epoch 2/15\n",
      "5000000/5000000 [==============================] - 88s 18us/step - loss: 0.5137 - acc: 0.7419\n",
      "Epoch 3/15\n",
      "5000000/5000000 [==============================] - 92s 18us/step - loss: 0.4928 - acc: 0.7554\n",
      "Epoch 4/15\n",
      "5000000/5000000 [==============================] - 95s 19us/step - loss: 0.4818 - acc: 0.7625\n",
      "Epoch 5/15\n",
      "5000000/5000000 [==============================] - 97s 19us/step - loss: 0.4749 - acc: 0.7669\n",
      "Epoch 6/15\n",
      "5000000/5000000 [==============================] - 97s 19us/step - loss: 0.4701 - acc: 0.7700\n",
      "Epoch 7/15\n",
      "5000000/5000000 [==============================] - 98s 20us/step - loss: 0.4662 - acc: 0.7725\n",
      "Epoch 8/15\n",
      "5000000/5000000 [==============================] - 98s 20us/step - loss: 0.4629 - acc: 0.7747\n",
      "Epoch 9/15\n",
      "5000000/5000000 [==============================] - 99s 20us/step - loss: 0.4601 - acc: 0.7765\n",
      "Epoch 10/15\n",
      "5000000/5000000 [==============================] - 92s 18us/step - loss: 0.4573 - acc: 0.7782\n",
      "Epoch 11/15\n",
      "5000000/5000000 [==============================] - 92s 18us/step - loss: 0.4551 - acc: 0.7795\n",
      "Epoch 12/15\n",
      "5000000/5000000 [==============================] - 87s 17us/step - loss: 0.4531 - acc: 0.7809\n",
      "Epoch 13/15\n",
      "5000000/5000000 [==============================] - 89s 18us/step - loss: 0.4512 - acc: 0.7819\n",
      "Epoch 14/15\n",
      "5000000/5000000 [==============================] - 92s 18us/step - loss: 0.4494 - acc: 0.7832\n",
      "Epoch 15/15\n",
      "5000000/5000000 [==============================] - 91s 18us/step - loss: 0.4477 - acc: 0.7842\n",
      "200000/200000 [==============================] - 1s 4us/step\n",
      "0.8616081784806315\n",
      "[0.4603735379874706, 0.7772049993276596]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(350, input_dim=x.shape[1], kernel_initializer='truncated_normal')) # X_train.shape[1] == 15 here\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(350, kernel_initializer='truncated_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(350, kernel_initializer='truncated_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='truncated_normal')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "nad = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=nad)\n",
    "\n",
    "model.fit(x, y, epochs=15, batch_size=5000)\n",
    "score = model.evaluate(x_test, y_test, batch_size=5000)\n",
    "print(roc_auc_score(y_test,model.predict(x_test)))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon the completion of the model with node sizes of 350 we found there to an increase in accuracy, by approximently .02, and increase in ROC Score, by approximently .2, and slight decrease in the loss value of .003. By increasing the node size by 150, or 75% over the previous model we saw an increase in time per Epoch approximently 60 seconds or 200% on average. With the increase model’s overall accuracy measures we continued to increase the node size to 750 to see if the model would continue to increase.\n",
    "\n",
    "### Increasing Node Size to 750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5000000/5000000 [==============================] - 313s 63us/step - loss: 0.5693 - acc: 0.6981\n",
      "Epoch 2/15\n",
      "5000000/5000000 [==============================] - 313s 63us/step - loss: 0.5057 - acc: 0.7475\n",
      "Epoch 3/15\n",
      "5000000/5000000 [==============================] - 326s 65us/step - loss: 0.4849 - acc: 0.7609\n",
      "Epoch 4/15\n",
      "5000000/5000000 [==============================] - 323s 65us/step - loss: 0.4741 - acc: 0.7679\n",
      "Epoch 5/15\n",
      "5000000/5000000 [==============================] - 325s 65us/step - loss: 0.4666 - acc: 0.7725\n",
      "Epoch 6/15\n",
      "5000000/5000000 [==============================] - 327s 65us/step - loss: 0.4607 - acc: 0.7764\n",
      "Epoch 7/15\n",
      "5000000/5000000 [==============================] - 325s 65us/step - loss: 0.4560 - acc: 0.7792\n",
      "Epoch 8/15\n",
      "5000000/5000000 [==============================] - 320s 64us/step - loss: 0.4510 - acc: 0.7823\n",
      "Epoch 9/15\n",
      "5000000/5000000 [==============================] - 321s 64us/step - loss: 0.4468 - acc: 0.7848\n",
      "Epoch 10/15\n",
      "5000000/5000000 [==============================] - 321s 64us/step - loss: 0.4425 - acc: 0.7875\n",
      "Epoch 11/15\n",
      "5000000/5000000 [==============================] - 319s 64us/step - loss: 0.4384 - acc: 0.7903\n",
      "Epoch 12/15\n",
      "5000000/5000000 [==============================] - 318s 64us/step - loss: 0.4341 - acc: 0.7927\n",
      "Epoch 13/15\n",
      "5000000/5000000 [==============================] - 320s 64us/step - loss: 0.4300 - acc: 0.7953\n",
      "Epoch 14/15\n",
      "5000000/5000000 [==============================] - 320s 64us/step - loss: 0.4257 - acc: 0.7977\n",
      "Epoch 15/15\n",
      "5000000/5000000 [==============================] - 319s 64us/step - loss: 0.4215 - acc: 0.8004\n",
      "200000/200000 [==============================] - 2s 12us/step\n",
      "0.8597218605430856\n",
      "[0.46676433756947516, 0.7760750010609627]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(750, input_dim=x.shape[1], kernel_initializer='truncated_normal')) # X_train.shape[1] == 15 here\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(750, kernel_initializer='truncated_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(750, kernel_initializer='truncated_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='truncated_normal')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "nad = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=nad)\n",
    "\n",
    "model.fit(x, y, epochs=15, batch_size=5000)\n",
    "score = model.evaluate(x_test, y_test, batch_size=5000)\n",
    "print(roc_auc_score(y_test,model.predict(x_test)))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon the completion of the final model using node sizes of 750 we find there to be a decrease in the models accuracy and ROC score. Additionally, the loss value was slightly higher, with the largest difference being the amount of time per Epoch. The average Epoch lasted 5.5 minutes, which is approximately 300% increase in run time when compared to the model with node sizes of 350. With the increase in time and resources, but a decrease in all the scoring attributes we chose to stop experiment and end with the model that gave us the highest results.\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The case study above used a number of different variables to attempt to create a neural network based on the Higgs Boson data set. One of the first variables we looked at was the creation of a model with different layers, our original model had five-layers with our second attempt having only four. The decrease in layers created a significant gain in both accuracy the ROC scores. For example, the second model with three layers had a ROC score of .7654 and the original five-layer model had a ROC score of .5730 respectively, which an approximate increase of .192. Additionally, the accuracy of the four-layer model is .6966 and the original five-layer model is .5333 respectively, which is an increase of approximately .1633. Through multiple iterations we found that by adding an extra layer to the model and using the original settings for initialization kernel and optimizer there was added noise, which the final result was a less accurate model with a larger loss function. To further test whether the four-layer model was the most efficient we tested a three-layer model, the code and results are available in the appendix, using the original settings with node sizes of 200 and the final settings with node sizes of 200 and 350. Overall the three-layer model performed better with the original settings, but when starting to alter additional features the model ended up performing slightly worse. With this in mind we chose to move forward in the case with using a 3-layer neural net with node sizes of 200. \n",
    "\n",
    "After deciding on a 3-layer model with node sizes of 200, we then set out to try multiple activation values. These three separate values were relu, tanh and linear. The results of this experiment were to use the relu activation, which resulted in a ROC score of .8528 and an accuracy of approximately .7688. These values were a dramatic increase from the original sigmoid activation that was previously attempted with an increase of approximately .09 for the model’s ROC score and a .07 increase in accuracy of the model. \n",
    "\n",
    "Next, we continued the testing of different variables by changing the initialization kernel that was used for each of the layers. For this case study we used the uniform, truncated_normal, VarianceScaling and Orthogonal initialization kernels. After multiple iterations we found that the truncated_normal initialization kernel gave us increased results over the other three values with a ROC score of .8544, which is an increase of approximately .002 and an approximately .0014 increase in the model’s accuracy when compared to a relu model with four-layers and a uniform initializer.  \n",
    "\n",
    "Finally, we tested multiple optimizer values, which where the Stochastic Gradient Descent or SGD,  RMSProp, Adagrad and Nadam. When compared to the model’s using SGD both the RMSProp and Adagrad models performed worse and created ROC and accuracy scores that were slightly less than the SGD model. The Nadam model produced the best model with a final ROC curve of .8544 and an accuracy score of .7702 respectively.\n",
    "\n",
    "In conclusion this case study took an extensive look at neural networks and how different settings can affect a model’s overall performance. After multiple iterations we found that a four-layer model with an activation of relu for the first three layers and sigmoid for the final layer, all with node sizes of 350 with initialization kernels of truncated_normal and a Nadam optimizer allowed us to find a final model with a ROC score of .8616 an accuracy score of .7772 and a loss value of .4704. Overall we found that the largest increase in the model’s accuracy came from the testing of different layers, with the second most being attributed to the changes in activation. \n",
    "\n",
    "\n",
    "## References \n",
    "1. https://home.cern/about/physics/standard-model\n",
    "2. https://home.cern/topics/higgs-boson\n",
    "3. https://www.cnn.com/2011/12/13/world/europe/higgs-boson-q-and-a/index.html\n",
    "4. https://en.wikipedia.org/wiki/TensorFlow\n",
    "5. https://keras.io/\n",
    "6. https://keras.io/getting-started/faq/#what-does-sample-batch-epoch-mean\n",
    "7. https://keras.io/initializers/\n",
    "8. http://ruder.io/optimizing-gradient-descent/index.html\n",
    "9. http://www.dataschool.io/roc-curves-and-auc-explained\n",
    "10. https://en.wikipedia.org/wiki/Cross-entropy\n",
    "\n",
    "## Appendix\n",
    "\n",
    "### Model 1 with relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.6576 - acc: 0.5882\n",
      "Epoch 2/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.5685 - acc: 0.7013\n",
      "Epoch 3/15\n",
      "5000000/5000000 [==============================] - 32s 6us/step - loss: 0.5368 - acc: 0.7250\n",
      "Epoch 4/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.5150 - acc: 0.7402\n",
      "Epoch 5/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.5011 - acc: 0.7494\n",
      "Epoch 6/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4930 - acc: 0.7547\n",
      "Epoch 7/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4875 - acc: 0.7582\n",
      "Epoch 8/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4838 - acc: 0.7606\n",
      "Epoch 9/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4805 - acc: 0.7627\n",
      "Epoch 10/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4778 - acc: 0.7647\n",
      "Epoch 11/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4756 - acc: 0.7660\n",
      "Epoch 12/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4737 - acc: 0.7674\n",
      "Epoch 13/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4719 - acc: 0.7684\n",
      "Epoch 14/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4702 - acc: 0.7696\n",
      "Epoch 15/15\n",
      "5000000/5000000 [==============================] - 31s 6us/step - loss: 0.4688 - acc: 0.7705\n",
      "200000/200000 [==============================] - 0s 2us/step\n",
      "0.85499839948284\n",
      "[0.4692159779369831, 0.7706750005483627]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=x.shape[1], kernel_initializer='uniform')) # X_train.shape[1] == 15 here\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200, kernel_initializer='uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200, kernel_initializer='uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(50, kernel_initializer='uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "model.fit(x, y, epochs=15, batch_size=5000)\n",
    "score = model.evaluate(x_test, y_test, batch_size=5000)\n",
    "print(roc_auc_score(y_test,model.predict(x_test)))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 with tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5000000/5000000 [==============================] - 55s 11us/step - loss: 0.6456 - acc: 0.6201\n",
      "Epoch 2/15\n",
      "5000000/5000000 [==============================] - 55s 11us/step - loss: 0.5984 - acc: 0.6783\n",
      "Epoch 3/15\n",
      "5000000/5000000 [==============================] - 55s 11us/step - loss: 0.5804 - acc: 0.6928\n",
      "Epoch 4/15\n",
      "5000000/5000000 [==============================] - 55s 11us/step - loss: 0.5689 - acc: 0.7015\n",
      "Epoch 5/15\n",
      "5000000/5000000 [==============================] - 55s 11us/step - loss: 0.5591 - acc: 0.7089\n",
      "Epoch 6/15\n",
      "5000000/5000000 [==============================] - 55s 11us/step - loss: 0.5504 - acc: 0.7154\n",
      "Epoch 7/15\n",
      "5000000/5000000 [==============================] - 55s 11us/step - loss: 0.5435 - acc: 0.7201\n",
      "Epoch 8/15\n",
      "5000000/5000000 [==============================] - 54s 11us/step - loss: 0.5376 - acc: 0.7243\n",
      "Epoch 9/15\n",
      "5000000/5000000 [==============================] - 55s 11us/step - loss: 0.5328 - acc: 0.7277\n",
      "Epoch 10/15\n",
      "5000000/5000000 [==============================] - 55s 11us/step - loss: 0.5268 - acc: 0.7319\n",
      "Epoch 11/15\n",
      "5000000/5000000 [==============================] - 55s 11us/step - loss: 0.5205 - acc: 0.7363\n",
      "Epoch 12/15\n",
      "5000000/5000000 [==============================] - 55s 11us/step - loss: 0.5148 - acc: 0.7404\n",
      "Epoch 13/15\n",
      "5000000/5000000 [==============================] - 55s 11us/step - loss: 0.5102 - acc: 0.7436\n",
      "Epoch 14/15\n",
      "5000000/5000000 [==============================] - 55s 11us/step - loss: 0.5061 - acc: 0.7462\n",
      "Epoch 15/15\n",
      "5000000/5000000 [==============================] - 55s 11us/step - loss: 0.5030 - acc: 0.7483\n",
      "200000/200000 [==============================] - 1s 7us/step\n",
      "0.8333002811939059\n",
      "[0.4996121749281883, 0.7516699999570846]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=x.shape[1], kernel_initializer='uniform')) # X_train.shape[1] == 15 here\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(200, kernel_initializer='uniform'))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(200, kernel_initializer='uniform'))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(50, kernel_initializer='uniform'))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(1, kernel_initializer='uniform')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "model.fit(x, y, epochs=15, batch_size=5000)\n",
    "score = model.evaluate(x_test, y_test, batch_size=5000)\n",
    "print(roc_auc_score(y_test,model.predict(x_test)))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 with linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5000000/5000000 [==============================] - 27s 5us/step - loss: 0.6498 - acc: 0.6160\n",
      "Epoch 2/15\n",
      "5000000/5000000 [==============================] - 27s 5us/step - loss: 0.6386 - acc: 0.6393\n",
      "Epoch 3/15\n",
      "5000000/5000000 [==============================] - 27s 5us/step - loss: 0.6384 - acc: 0.6399\n",
      "Epoch 4/15\n",
      "5000000/5000000 [==============================] - 27s 5us/step - loss: 0.6383 - acc: 0.6401\n",
      "Epoch 5/15\n",
      "5000000/5000000 [==============================] - 27s 5us/step - loss: 0.6383 - acc: 0.6403\n",
      "Epoch 6/15\n",
      "5000000/5000000 [==============================] - 27s 5us/step - loss: 0.6382 - acc: 0.6404\n",
      "Epoch 7/15\n",
      "5000000/5000000 [==============================] - 27s 5us/step - loss: 0.6382 - acc: 0.6404\n",
      "Epoch 8/15\n",
      "5000000/5000000 [==============================] - 27s 5us/step - loss: 0.6382 - acc: 0.6405\n",
      "Epoch 9/15\n",
      "5000000/5000000 [==============================] - 27s 5us/step - loss: 0.6382 - acc: 0.6406\n",
      "Epoch 10/15\n",
      "5000000/5000000 [==============================] - 27s 5us/step - loss: 0.6381 - acc: 0.6406\n",
      "Epoch 11/15\n",
      "5000000/5000000 [==============================] - 27s 5us/step - loss: 0.6382 - acc: 0.6406\n",
      "Epoch 12/15\n",
      "5000000/5000000 [==============================] - 27s 5us/step - loss: 0.6381 - acc: 0.6407\n",
      "Epoch 13/15\n",
      "5000000/5000000 [==============================] - 27s 5us/step - loss: 0.6381 - acc: 0.6407\n",
      "Epoch 14/15\n",
      "5000000/5000000 [==============================] - 27s 5us/step - loss: 0.6381 - acc: 0.6406\n",
      "Epoch 15/15\n",
      "5000000/5000000 [==============================] - 27s 5us/step - loss: 0.6381 - acc: 0.6408\n",
      "200000/200000 [==============================] - 0s 2us/step\n",
      "0.6847885730110183\n",
      "[0.6370486095547676, 0.6418449997901916]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=x.shape[1], kernel_initializer='uniform')) # X_train.shape[1] == 15 here\n",
    "model.add(Activation('linear'))\n",
    "model.add(Dense(200, kernel_initializer='uniform'))\n",
    "model.add(Activation('linear'))\n",
    "model.add(Dense(200, kernel_initializer='uniform'))\n",
    "model.add(Activation('linear'))\n",
    "model.add(Dense(50, kernel_initializer='uniform'))\n",
    "model.add(Activation('linear'))\n",
    "model.add(Dense(1, kernel_initializer='uniform')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "model.fit(x, y, epochs=15, batch_size=5000)\n",
    "score = model.evaluate(x_test, y_test, batch_size=5000)\n",
    "print(roc_auc_score(y_test,model.predict(x_test)))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 with relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5000000/5000000 [==============================] - 20s 4us/step - loss: 0.6179 - acc: 0.6465\n",
      "Epoch 2/15\n",
      "5000000/5000000 [==============================] - 20s 4us/step - loss: 0.5560 - acc: 0.7107\n",
      "Epoch 3/15\n",
      "5000000/5000000 [==============================] - 20s 4us/step - loss: 0.5312 - acc: 0.7288\n",
      "Epoch 4/15\n",
      "5000000/5000000 [==============================] - 20s 4us/step - loss: 0.5139 - acc: 0.7409\n",
      "Epoch 5/15\n",
      "5000000/5000000 [==============================] - 20s 4us/step - loss: 0.5024 - acc: 0.7487\n",
      "Epoch 6/15\n",
      "5000000/5000000 [==============================] - 20s 4us/step - loss: 0.4954 - acc: 0.7534\n",
      "Epoch 7/15\n",
      "5000000/5000000 [==============================] - 20s 4us/step - loss: 0.4908 - acc: 0.7563\n",
      "Epoch 8/15\n",
      "5000000/5000000 [==============================] - 20s 4us/step - loss: 0.4871 - acc: 0.7589\n",
      "Epoch 9/15\n",
      "5000000/5000000 [==============================] - 20s 4us/step - loss: 0.4845 - acc: 0.7603\n",
      "Epoch 10/15\n",
      "5000000/5000000 [==============================] - 20s 4us/step - loss: 0.4822 - acc: 0.7619\n",
      "Epoch 11/15\n",
      "5000000/5000000 [==============================] - 20s 4us/step - loss: 0.4803 - acc: 0.7632\n",
      "Epoch 12/15\n",
      "5000000/5000000 [==============================] - 20s 4us/step - loss: 0.4787 - acc: 0.7641\n",
      "Epoch 13/15\n",
      "5000000/5000000 [==============================] - 20s 4us/step - loss: 0.4773 - acc: 0.7651\n",
      "Epoch 14/15\n",
      "5000000/5000000 [==============================] - 21s 4us/step - loss: 0.4760 - acc: 0.7658\n",
      "Epoch 15/15\n",
      "5000000/5000000 [==============================] - 20s 4us/step - loss: 0.4749 - acc: 0.7665\n",
      "200000/200000 [==============================] - 0s 1us/step\n",
      "0.8512386947040244\n",
      "[0.4747511692345142, 0.7681350007653236]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(150, input_dim=x.shape[1], kernel_initializer='uniform')) # X_train.shape[1] == 15 here\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(150, kernel_initializer='uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(150, kernel_initializer='uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "model.fit(x, y, epochs=15, batch_size=5000)\n",
    "score = model.evaluate(x_test, y_test, batch_size=5000)\n",
    "print(roc_auc_score(y_test,model.predict(x_test)))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 with tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5000000/5000000 [==============================] - 37s 7us/step - loss: 0.6352 - acc: 0.6354\n",
      "Epoch 2/15\n",
      "5000000/5000000 [==============================] - 37s 7us/step - loss: 0.5951 - acc: 0.6815\n",
      "Epoch 3/15\n",
      "5000000/5000000 [==============================] - 37s 7us/step - loss: 0.5796 - acc: 0.6932\n",
      "Epoch 4/15\n",
      "5000000/5000000 [==============================] - 37s 7us/step - loss: 0.5685 - acc: 0.7011\n",
      "Epoch 5/15\n",
      "5000000/5000000 [==============================] - 37s 7us/step - loss: 0.5596 - acc: 0.7078\n",
      "Epoch 6/15\n",
      "5000000/5000000 [==============================] - 37s 7us/step - loss: 0.5528 - acc: 0.7131\n",
      "Epoch 7/15\n",
      "5000000/5000000 [==============================] - 37s 7us/step - loss: 0.5459 - acc: 0.7182\n",
      "Epoch 8/15\n",
      "5000000/5000000 [==============================] - 37s 7us/step - loss: 0.5393 - acc: 0.7233\n",
      "Epoch 9/15\n",
      "5000000/5000000 [==============================] - 37s 7us/step - loss: 0.5343 - acc: 0.7270\n",
      "Epoch 10/15\n",
      "5000000/5000000 [==============================] - 37s 7us/step - loss: 0.5299 - acc: 0.7300\n",
      "Epoch 11/15\n",
      "5000000/5000000 [==============================] - 37s 7us/step - loss: 0.5257 - acc: 0.7332\n",
      "Epoch 12/15\n",
      "5000000/5000000 [==============================] - 37s 7us/step - loss: 0.5212 - acc: 0.7365\n",
      "Epoch 13/15\n",
      "5000000/5000000 [==============================] - 37s 7us/step - loss: 0.5169 - acc: 0.7395\n",
      "Epoch 14/15\n",
      "5000000/5000000 [==============================] - 37s 7us/step - loss: 0.5124 - acc: 0.7427\n",
      "Epoch 15/15\n",
      "5000000/5000000 [==============================] - 37s 7us/step - loss: 0.5089 - acc: 0.7448\n",
      "200000/200000 [==============================] - 1s 4us/step\n",
      "0.8294619663828303\n",
      "[0.504516588151455, 0.7482000023126603]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(150, input_dim=x.shape[1], kernel_initializer='uniform')) # X_train.shape[1] == 15 here\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(150, kernel_initializer='uniform'))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(150, kernel_initializer='uniform'))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(1, kernel_initializer='uniform')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "model.fit(x, y, epochs=15, batch_size=5000)\n",
    "score = model.evaluate(x_test, y_test, batch_size=5000)\n",
    "print(roc_auc_score(y_test,model.predict(x_test)))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 with linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5000000/5000000 [==============================] - 18s 4us/step - loss: 0.6458 - acc: 0.6240\n",
      "Epoch 2/15\n",
      "5000000/5000000 [==============================] - 18s 4us/step - loss: 0.6385 - acc: 0.6395\n",
      "Epoch 3/15\n",
      "5000000/5000000 [==============================] - 18s 4us/step - loss: 0.6384 - acc: 0.6399\n",
      "Epoch 4/15\n",
      "5000000/5000000 [==============================] - 18s 4us/step - loss: 0.6383 - acc: 0.6402\n",
      "Epoch 5/15\n",
      "5000000/5000000 [==============================] - 18s 4us/step - loss: 0.6383 - acc: 0.6402\n",
      "Epoch 6/15\n",
      "5000000/5000000 [==============================] - 18s 4us/step - loss: 0.6382 - acc: 0.6404\n",
      "Epoch 7/15\n",
      "5000000/5000000 [==============================] - 18s 4us/step - loss: 0.6382 - acc: 0.6405\n",
      "Epoch 8/15\n",
      "5000000/5000000 [==============================] - 18s 4us/step - loss: 0.6382 - acc: 0.6406\n",
      "Epoch 9/15\n",
      "5000000/5000000 [==============================] - 18s 4us/step - loss: 0.6382 - acc: 0.6406\n",
      "Epoch 10/15\n",
      "5000000/5000000 [==============================] - 18s 4us/step - loss: 0.6381 - acc: 0.6406\n",
      "Epoch 11/15\n",
      "5000000/5000000 [==============================] - 18s 4us/step - loss: 0.6381 - acc: 0.6407\n",
      "Epoch 12/15\n",
      "5000000/5000000 [==============================] - 18s 4us/step - loss: 0.6381 - acc: 0.6407\n",
      "Epoch 13/15\n",
      "5000000/5000000 [==============================] - 18s 4us/step - loss: 0.6381 - acc: 0.6407\n",
      "Epoch 14/15\n",
      "5000000/5000000 [==============================] - 18s 4us/step - loss: 0.6381 - acc: 0.6407\n",
      "Epoch 15/15\n",
      "5000000/5000000 [==============================] - 18s 4us/step - loss: 0.6381 - acc: 0.6407\n",
      "200000/200000 [==============================] - 0s 1us/step\n",
      "0.6851573061059237\n",
      "[0.6369462415575982, 0.642044997215271]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(150, input_dim=x.shape[1], kernel_initializer='uniform')) # X_train.shape[1] == 15 here\n",
    "model.add(Activation('linear'))\n",
    "model.add(Dense(150, kernel_initializer='uniform'))\n",
    "model.add(Activation('linear'))\n",
    "model.add(Dense(150, kernel_initializer='uniform'))\n",
    "model.add(Activation('linear'))\n",
    "model.add(Dense(1, kernel_initializer='uniform')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "model.fit(x, y, epochs=15, batch_size=5000)\n",
    "score = model.evaluate(x_test, y_test, batch_size=5000)\n",
    "print(roc_auc_score(y_test,model.predict(x_test)))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with 3-layers and Node size of 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5000000/5000000 [==============================] - 38s 8us/step - loss: 0.6817 - acc: 0.5520\n",
      "Epoch 2/15\n",
      "5000000/5000000 [==============================] - 41s 8us/step - loss: 0.6445 - acc: 0.6238\n",
      "Epoch 3/15\n",
      "5000000/5000000 [==============================] - 39s 8us/step - loss: 0.6383 - acc: 0.6365\n",
      "Epoch 4/15\n",
      "5000000/5000000 [==============================] - 39s 8us/step - loss: 0.6304 - acc: 0.6492\n",
      "Epoch 5/15\n",
      "5000000/5000000 [==============================] - 39s 8us/step - loss: 0.6168 - acc: 0.6636\n",
      "Epoch 6/15\n",
      "5000000/5000000 [==============================] - 40s 8us/step - loss: 0.6027 - acc: 0.6764\n",
      "Epoch 7/15\n",
      "5000000/5000000 [==============================] - 40s 8us/step - loss: 0.5948 - acc: 0.6827\n",
      "Epoch 8/15\n",
      "5000000/5000000 [==============================] - 40s 8us/step - loss: 0.5898 - acc: 0.6867\n",
      "Epoch 9/15\n",
      "5000000/5000000 [==============================] - 41s 8us/step - loss: 0.5856 - acc: 0.6898\n",
      "Epoch 10/15\n",
      "5000000/5000000 [==============================] - 39s 8us/step - loss: 0.5821 - acc: 0.6925\n",
      "Epoch 11/15\n",
      "5000000/5000000 [==============================] - 40s 8us/step - loss: 0.5789 - acc: 0.6948\n",
      "Epoch 12/15\n",
      "5000000/5000000 [==============================] - 40s 8us/step - loss: 0.5761 - acc: 0.6967\n",
      "Epoch 13/15\n",
      "5000000/5000000 [==============================] - 40s 8us/step - loss: 0.5734 - acc: 0.6987\n",
      "Epoch 14/15\n",
      "5000000/5000000 [==============================] - 41s 8us/step - loss: 0.5710 - acc: 0.7006\n",
      "Epoch 15/15\n",
      "5000000/5000000 [==============================] - 40s 8us/step - loss: 0.5686 - acc: 0.7024\n",
      "200000/200000 [==============================] - 1s 5us/step\n",
      "0.7795043947435436\n",
      "[0.5631455138325692, 0.7060449972748757]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=x.shape[1], kernel_initializer='uniform')) # X_train.shape[1] == 15 here\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(200, kernel_initializer='uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='uniform')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "model.fit(x, y, epochs=15, batch_size=5000)\n",
    "score = model.evaluate(x_test, y_test, batch_size=5000)\n",
    "print(roc_auc_score(y_test,model.predict(x_test)))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-layer model with Node Size 200 and final settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5000000/5000000 [==============================] - 22s 4us/step - loss: 0.5878 - acc: 0.6825\n",
      "Epoch 2/15\n",
      "5000000/5000000 [==============================] - 24s 5us/step - loss: 0.5420 - acc: 0.7229\n",
      "Epoch 3/15\n",
      "5000000/5000000 [==============================] - 26s 5us/step - loss: 0.5227 - acc: 0.7366\n",
      "Epoch 4/15\n",
      "5000000/5000000 [==============================] - 25s 5us/step - loss: 0.5098 - acc: 0.7452\n",
      "Epoch 5/15\n",
      "5000000/5000000 [==============================] - 22s 4us/step - loss: 0.5012 - acc: 0.7505\n",
      "Epoch 6/15\n",
      "5000000/5000000 [==============================] - 22s 4us/step - loss: 0.4950 - acc: 0.7543\n",
      "Epoch 7/15\n",
      "5000000/5000000 [==============================] - 22s 4us/step - loss: 0.4902 - acc: 0.7576\n",
      "Epoch 8/15\n",
      "5000000/5000000 [==============================] - 24s 5us/step - loss: 0.4866 - acc: 0.7597\n",
      "Epoch 9/15\n",
      "5000000/5000000 [==============================] - 24s 5us/step - loss: 0.4841 - acc: 0.7613\n",
      "Epoch 10/15\n",
      "5000000/5000000 [==============================] - 25s 5us/step - loss: 0.4818 - acc: 0.7626\n",
      "Epoch 11/15\n",
      "5000000/5000000 [==============================] - 24s 5us/step - loss: 0.4800 - acc: 0.7638\n",
      "Epoch 12/15\n",
      "5000000/5000000 [==============================] - 25s 5us/step - loss: 0.4783 - acc: 0.7650\n",
      "Epoch 13/15\n",
      "5000000/5000000 [==============================] - 25s 5us/step - loss: 0.4769 - acc: 0.7657\n",
      "Epoch 14/15\n",
      "5000000/5000000 [==============================] - 25s 5us/step - loss: 0.4757 - acc: 0.7665\n",
      "Epoch 15/15\n",
      "5000000/5000000 [==============================] - 25s 5us/step - loss: 0.4746 - acc: 0.7673\n",
      "200000/200000 [==============================] - 0s 2us/step\n",
      "0.8503651488793429\n",
      "[0.47695332765579224, 0.7665249973535537]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=x.shape[1], kernel_initializer='truncated_normal')) # X_train.shape[1] == 15 here\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200, kernel_initializer='truncated_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='truncated_normal')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "nad = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=nad)\n",
    "\n",
    "model.fit(x, y, epochs=15, batch_size=5000)\n",
    "score = model.evaluate(x_test, y_test, batch_size=5000)\n",
    "print(roc_auc_score(y_test,model.predict(x_test)))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-layer model with Node Size 350 and final settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5000000/5000000 [==============================] - 54s 11us/step - loss: 0.5791 - acc: 0.6901\n",
      "Epoch 2/15\n",
      "5000000/5000000 [==============================] - 57s 11us/step - loss: 0.5276 - acc: 0.7334\n",
      "Epoch 3/15\n",
      "5000000/5000000 [==============================] - 55s 11us/step - loss: 0.5077 - acc: 0.7468\n",
      "Epoch 4/15\n",
      "5000000/5000000 [==============================] - 55s 11us/step - loss: 0.4966 - acc: 0.7539\n",
      "Epoch 5/15\n",
      "5000000/5000000 [==============================] - 55s 11us/step - loss: 0.4892 - acc: 0.7585\n",
      "Epoch 6/15\n",
      "5000000/5000000 [==============================] - 56s 11us/step - loss: 0.4842 - acc: 0.7614\n",
      "Epoch 7/15\n",
      "5000000/5000000 [==============================] - 56s 11us/step - loss: 0.4803 - acc: 0.7642\n",
      "Epoch 8/15\n",
      "5000000/5000000 [==============================] - 56s 11us/step - loss: 0.4774 - acc: 0.7659\n",
      "Epoch 9/15\n",
      "5000000/5000000 [==============================] - 58s 12us/step - loss: 0.4749 - acc: 0.7676\n",
      "Epoch 10/15\n",
      "5000000/5000000 [==============================] - 58s 12us/step - loss: 0.4727 - acc: 0.7688\n",
      "Epoch 11/15\n",
      "5000000/5000000 [==============================] - 58s 12us/step - loss: 0.4709 - acc: 0.7701\n",
      "Epoch 12/15\n",
      "5000000/5000000 [==============================] - 58s 12us/step - loss: 0.4691 - acc: 0.7712\n",
      "Epoch 13/15\n",
      "5000000/5000000 [==============================] - 58s 12us/step - loss: 0.4676 - acc: 0.7721\n",
      "Epoch 14/15\n",
      "5000000/5000000 [==============================] - 58s 12us/step - loss: 0.4660 - acc: 0.7731\n",
      "Epoch 15/15\n",
      "5000000/5000000 [==============================] - 59s 12us/step - loss: 0.4648 - acc: 0.7737\n",
      "200000/200000 [==============================] - 1s 3us/step\n",
      "0.8555201526011412\n",
      "[0.4690599337220192, 0.7713600009679794]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(350, input_dim=x.shape[1], kernel_initializer='truncated_normal')) # X_train.shape[1] == 15 here\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(350, kernel_initializer='truncated_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='truncated_normal')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "nad = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=nad)\n",
    "\n",
    "model.fit(x, y, epochs=15, batch_size=5000)\n",
    "score = model.evaluate(x_test, y_test, batch_size=5000)\n",
    "print(roc_auc_score(y_test,model.predict(x_test)))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
